In this project, I performed a comprehensive data analysis on two CSV files (task2a.csv and task2b.csv) that contained information about the prevalence of various mental disorders across different countries and years. The project consisted of five functional requirements (FRs), each with a different objective and difficulty level.

## FR6: 
The first requirement was to read CSV data from two files and `merge` it into a single data frame using the `pandas` library. This was an easy task, as I only had to use the `pd.read_csv` and `pd.merge` functions to read and combine the data from the two files.

## FR7: 
The second requirement was to explore the dataset to identify an “interesting” pattern or trend using an appropriate visualization tool. This was the most challenging and exciting part of the project, as I had to use my creativity and curiosity to find meaningful insights from the data. I started by creating a `heatmap` using the `seaborn` library to visualize the `correlations` between the different mental disorders. This revealed some strong and weak relationships that caught my attention. I then decided to focus on the `correlation` between eating disorders and bipolar disorders across different countries, as I found it to be intriguing and significant. I used the `matplotlib` library to create `line charts` to show the prevalence of these disorders over time for the countries with the highest and lowest rates. This uncovered some fascinating patterns, such as the inverse relationship between eating disorders and bipolar disorders in Japan and the Philippines, and the positive relationship between them in Norway and Sweden. I also explored other disorders, such as depression, schizophrenia, and anxiety disorders, and compared their prevalence across different countries and years. I learned how to use various data visualization techniques, such as `heatmaps`, `line charts`, `scatter plots`, and `bar charts`, to effectively communicate my findings and support my analysis.

## FR8: 
The third requirement was to detect and remove any `outliers` in the data used for my “interesting” pattern or trend using an appropriate technique. This was a relatively easy task, as I had learned how to use `boxplots` to identify outliers in a previous session. I used the `seaborn` library to create boxplots for the data related to eating disorders and bipolar disorders, and visually inspected them for any extreme values. I found some outliers in both variables, and decided to remove them using the `Interquartile Range` (IQR) method. This method involves calculating the difference between the 75th and 25th percentiles of the data, and then defining the upper and lower bounds as 1.5 times the IQR above and below the percentiles, respectively. Any value that falls outside these bounds is considered an outlier and removed from the data. I used the `pandas` library to implement this method and filter out the outliers from the data frame. I then plotted the data again to compare the results before and after the outlier removal. This showed me how outliers can affect the distribution and shape of the data, and how removing them can improve the accuracy and reliability of the analysis.

## FR9: 
The fourth requirement was to define a `hypothesis` to test my “interesting” pattern using an appropriate hypothesis testing formulation. This was a new and interesting task for me, as I had to apply my statistical knowledge and reasoning to formulate a testable hypothesis. I learned that a hypothesis test is a method of making a decision or inference about a population parameter based on a sample statistic. I decided to test the relationship between drug use disorders and other disorders, as well as alcohol use disorders and other disorders, as I found them to be relevant and important for the data analysis. I used the `Pearson correlation coefficient` as the test statistic, which measures the linear association between two variables. I formulated the `null hypothesis` as the assumption that there is no relationship between the variables, and the `alternative hypothesis` as the claim that there is a relationship between the variables. I also chose a significance level of 0.05, which represents the probability of rejecting the null hypothesis when it is true.

## FR10: 
The fifth and final requirement was to test my hypothesis with a statistical significance level of 0.05 using an appropriate Python library. This was a straightforward task, as I could use the `scipy` library to perform the hypothesis test. I used the `scipy.stats.pearsonr` function to calculate the `Pearson correlation coefficient` and the `p-value` for each pair of variables that I wanted to test. The p-value is the probability of obtaining a test statistic at least as extreme as the observed one, assuming that the null hypothesis is true. I compared the p-value with the significance level, and made a decision based on the following rule: if the p-value is less than or equal to the significance level, reject the null hypothesis and accept the alternative hypothesis; otherwise, fail to reject the null hypothesis. The results of the hypothesis test showed that there was a statistically significant relationship between drug use disorders and other disorders, as well as alcohol use disorders and other disorders, for most of the pairs that I tested. This confirmed my findings from the data exploration and visualization, and provided valuable insights into the data.


This project was a rewarding learning experience for me, as it challenged me to use various Python libraries and techniques to perform a comprehensive data analysis on CSV data. I learned how to read and merge CSV data using pandas, how to explore and visualize data using seaborn and matplotlib, how to detect and remove outliers using boxplots and IQR method, how to formulate and test hypotheses using scipy and Pearson correlation, and how to communicate and interpret my findings and insights. I also improved my data analysis and visualization skills and my problem-solving abilities.
